## How Am I Doing?

### Your Peers and Your Performance

We have two formalized methods of evaluating each other: peer reviews and stack ranking. Peer reviews are done in order to give each other useful feedback on how to best grow as individual contributors. Stack ranking is done primarily as a method of adjusting compensation. Both processes are driven by information gathered from each other — your peers.

#### Peer reviews

We all need feedback about our performance—in order to improve, and in order to know we’re not failing. Once a year we all give each other feedback about our work.  Outside of these formalized peer reviews, the expectation is that we’ll just pull feedback from those around us whenever we need to.

There is a framework for how we give this feedback to each other. A set of people (the set changes each time) interviews everyone in the whole company, asking who each person has worked with since the last round of peer reviews and how the experience of working with each person was. The purpose of the feedback is to provide people with information that will help them grow. That means that the best quality feedback is directive and prescriptive, and designed to be put to use by the person you’re talking about.

The feedback is then gathered, collated, anonymized, and delivered to each reviewee. Making the feedback anonymous definitely has pros and cons, but we think it’s the best way to get the most useful information to each person. There’s no reason to keep your feedback about someone to yourself until peer review time if you’d like to deliver it sooner. In fact, it’s much better if you do so often, and outside the constraints of official peer reviews.

When delivering peer review feedback, it’s useful to keep in mind the same categories used in stack ranking because they concretely measure how valuable we think someone is.


### Stack ranking (and compensation)

The other evaluation we do annually is to rank each other against our peers. Unlike peer reviews, which generate information for each individual, stack ranking is done in order to gain insight into who’s providing the most value at the company and to thereby adjust each person’s compensation to be commensurate with his or her actual value.

Valve pays people very well compared to industry norms.  Our profitability per employee is higher than that of Google or Amazon or Microsoft, and we believe strongly that the right thing to do in that case is to put a maximum amount of money back into each employee’s pocket. Valve does not win if you’re paid less than the value you create.  And people who work here ultimately don’t win if they get paid more than the value they create.

So Valve’s goal is to get your compensation to be “correct.” We tend to be very flexible when new employees are joining the company, listening to their salary requirements and doing what we can for them. Over time, compensation gets adjusted to fit an employee’s internal peer-driven valuation.  That’s what we mean by “correct”—paying someone what they’re worth (as best we can tell using the opinions of peers).

  If you think your compensation isn’t right for the work you do, then you should raise the issue. At Valve, these conversations are surprisingly easy and straightforward. Adjustments to compensation usually occur within the process described here. But talking about it is always the right thing if there’s any issue. Fretting about your level of compensation without any outside information about how it got set is expensive for you and for Valve.

The removal of bias is of the utmost importance to Valve in this process. We believe that our peers are the best judges of our value as individuals. Our flat structure eliminates some of the bias that would be present in a peer-ranking system elsewhere. The design of our stack-ranking process is meant to eliminate as much as possible of the remainder.

Each project/product group is asked to rank its own members. (People are not asked to rank themselves, so we split groups into parts, and then each part ranks people other than themselves.) The ranking itself is based on the following four metrics:

  1. Skill Level/Technical Ability
  How difficult and valuable are the kinds of problems you solve? How important/critical of a problem can you be given? Are you uniquely capable (in the company?  industry?) of solving a certain class of problem, delivering a certain type of art asset, contributing to design, writing, or music, etc.?

  2. Productivity/Output
How much shippable (not necessarily shipped to outside customers), valuable, finished work did you get done?  Working a lot of hours is generally not related to productivity and, after a certain point, indicates inefficiency.  It is more valuable if you are able to maintain a sensible work/life balance and use your time in the office efficiently, rather than working around the clock.

  3. Group Contribution
  How much do you contribute to studio process, hiring, integrating people into the team, improving workflow, amplifying your colleagues, or writing tools used by others? Generally, being a group contributor means that you are making a tradeoff versus an individual contribution. Stepping up and acting in a leadership role can be good for your group contribution score, but being a leader does not impart or guarantee a higher stack rank. It is just a role that people adopt from time to time.

  4. Product Contribution
  How much do you contribute at a larger scope than your core skill? How much of your work matters to the product?  How much did you influence correct prioritization of work or resource trade-offs by others? Are you good at predicting how customers are going to react to decisions we’re making? Things like being a good playtester or bug finder during the shipping cycle would fall into this category.

